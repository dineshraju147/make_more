{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MakeMore Version 2: Neural Probabilistic Language Model with Character Embeddings\n",
    "#### Description:\n",
    "- This version builds on the original bigram model by introducing a multi-layer perceptron (MLP) to learn character-level language modeling using neural networks.\n",
    "- It leverages an embedding layer to represent characters in a lower-dimensional space and trains a simple neural network to predict the next character in a name sequence.\n",
    "- Compared to Version 1, this version moves from counting-based statistics to learnable parameters and backpropagation.\n"
   ],
   "id": "18dafea22c9c1c3a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-20T18:03:43.399633Z",
     "start_time": "2025-07-20T18:03:43.308026Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "\n",
    "%matplotlib inline\n"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ“¦ Data Preparation\n",
    "\n",
    "We begin by loading the dataset and encoding names as sequences of integer character indices. The character vocabulary includes 26 letters + a special token \".\".\n",
    "\n",
    "We will extract training examples of 3-character blocks (context) used to predict the next character.\n"
   ],
   "id": "18325783864d0d59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:04:40.144467Z",
     "start_time": "2025-07-20T18:04:40.113626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ],
   "id": "8a2b89a8ca48a0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:04:46.305388Z",
     "start_time": "2025-07-20T18:04:46.296931Z"
    }
   },
   "cell_type": "code",
   "source": "len(words)",
   "id": "b3d1244c552afb16",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:04:53.933198Z",
     "start_time": "2025-07-20T18:04:53.922812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ],
   "id": "fb6993bd6009f0fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:45:11.687235Z",
     "start_time": "2025-07-20T18:45:11.232955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 3\n",
    "context = [0] * block_size\n",
    "\n",
    "X , Y = [] , []\n",
    "\n",
    "for w in words:\n",
    "    # print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + \".\":\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        # print(\"X:\" , X ,  \"Y:\" , Y)\n",
    "        # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n"
   ],
   "id": "e9260b3b47607686",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:06:39.593146Z",
     "start_time": "2025-07-20T18:06:39.589182Z"
    }
   },
   "cell_type": "code",
   "source": "X.shape,X.dtype,  Y.shape, Y.dtype",
   "id": "bea1a2397270b17e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Lets Build our Embedding Table (Look Up Table) for our Character level model",
   "id": "df95833683a67dd1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Now we have make our dataset to predict the probability of nxt word. lets put these into neural network layer. Before moving to feeding, we need to reduce our 27 char we have into lower dimensions --> Two dim\n",
    "-- As we are proceeding to reimplement the architecture by A Neural Probabilistic Language Model {A Neural Probabilistic Language Model}\n",
    "-- they have implemented via 17000  words in our case we are proceeding with char level model so we have 27 dim\n",
    "-- They compressed 17000 dim to 30 dim , So lets compress the dimension to 2."
   ],
   "id": "fb69259d7bc80e3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:06:41.138454Z",
     "start_time": "2025-07-20T18:06:41.130020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Look Up Table : -- C\n",
    "# by the way these are the weights which we adjust during the back-propagation\n",
    "C = torch.rand(27, 2) # intialize random numbers for Look up table\n",
    "C"
   ],
   "id": "e27fe39fcef985ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2170, 0.7707],\n",
       "        [0.2801, 0.6671],\n",
       "        [0.9972, 0.9303],\n",
       "        [0.8709, 0.1039],\n",
       "        [0.7985, 0.2169],\n",
       "        [0.1397, 0.9650],\n",
       "        [0.6289, 0.9597],\n",
       "        [0.6178, 0.9784],\n",
       "        [0.4439, 0.0642],\n",
       "        [0.8414, 0.3987],\n",
       "        [0.4577, 0.2143],\n",
       "        [0.3336, 0.3532],\n",
       "        [0.6804, 0.3702],\n",
       "        [0.4153, 0.2636],\n",
       "        [0.9702, 0.2330],\n",
       "        [0.2867, 0.5706],\n",
       "        [0.0337, 0.2609],\n",
       "        [0.7708, 0.3116],\n",
       "        [0.6444, 0.6671],\n",
       "        [0.9819, 0.5261],\n",
       "        [0.7613, 0.5661],\n",
       "        [0.2441, 0.4689],\n",
       "        [0.3162, 0.2558],\n",
       "        [0.3457, 0.4985],\n",
       "        [0.9894, 0.7684],\n",
       "        [0.1325, 0.8925],\n",
       "        [0.1049, 0.3822]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:06:42.879173Z",
     "start_time": "2025-07-20T18:06:42.870443Z"
    }
   },
   "cell_type": "code",
   "source": "C[5]",
   "id": "db8458ec2bf1bac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1397, 0.9650])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:06:44.323665Z",
     "start_time": "2025-07-20T18:06:44.313837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Generate Character Embeddings\n",
    "\n",
    "# Suppose we want to encode the character represented by index 5 in a vocabulary of 27 characters.\n",
    "\n",
    "# Why do we encode it this way?\n",
    "# --- To generate a dense embedding for each character, we first convert the index into a one-hot vector of size 27.\n",
    "# --- Then, we extract its corresponding embedding vector by performing a matrix multiplication with the embedding table (lookup table C).\n",
    "\n",
    "F.one_hot(torch.tensor([5 ]), 27).dtype"
   ],
   "id": "1a6dd5decb8ba604",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:06:45.324057Z",
     "start_time": "2025-07-20T18:06:45.313210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Using this method, we can extract the relevant character embedding to feed into the neural network.\n",
    "# While this works, PyTorch provides a much simpler and more efficient way to handle embeddings.\n",
    "# Instead of manually one-hot encoding and multiplying, we can directly index into the embedding layer using the character indices.\n",
    "F.one_hot(torch.tensor([5 ]), 27).float() @ C\n"
   ],
   "id": "83959bdef3d59ff9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1397, 0.9650]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:06:47.594744Z",
     "start_time": "2025-07-20T18:06:47.580448Z"
    }
   },
   "cell_type": "code",
   "source": "C[torch.tensor([5, 6, 7,7,7  ])]",
   "id": "4de56dbc7ebdb8b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1397, 0.9650],\n",
       "        [0.6289, 0.9597],\n",
       "        [0.6178, 0.9784],\n",
       "        [0.6178, 0.9784],\n",
       "        [0.6178, 0.9784]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:29:04.138032Z",
     "start_time": "2025-07-20T18:29:04.108038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We can also pass a multi-dimensional tensor of indices (e.g., shape (32, 3)) to retrieve a batch of embeddings.\n",
    "# In this case:\n",
    "# - X is a (32, 3) tensor containing character indices.\n",
    "# - C is our embedding table of shape (27, 2), where 27 is the vocab size and 2 is the embedding dimension.\n",
    "# - When we index C with X (i.e., C[X]), the resulting shape is (32, 3, 2), giving us an embedding vector for each character in the input tensor.\n",
    "C[X].shape"
   ],
   "id": "cc3bcd43e54a953a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 3, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:06:51.612924Z",
     "start_time": "2025-07-20T18:06:51.608166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# lets check this\n",
    "X[13, 2]"
   ],
   "id": "b06a4de64fe46d01",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:06:52.855208Z",
     "start_time": "2025-07-20T18:06:52.847655Z"
    }
   },
   "cell_type": "code",
   "source": "C[X][13, 2]",
   "id": "2ae8b4e2b1fc81bb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2801, 0.6671])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:06:54.117880Z",
     "start_time": "2025-07-20T18:06:54.112453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# That's how its embed on the 3 dim array\n",
    "C[1]"
   ],
   "id": "a0c392045b8e9d2f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2801, 0.6671])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:32:32.921127Z",
     "start_time": "2025-07-20T18:32:32.835473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This is our embedding matrix for the characters.\n",
    "embed = C[X]\n",
    "print(embed.shape)  # Shape: (32, 3, 2)\n",
    "\n",
    "# Let's begin with the first linear (fully connected) layer of our MLP.\n",
    "W1 = torch.randn(6, 100)  # Weight matrix: input_dim=6, output_dim=100\n",
    "b1 = torch.randn(100)     # Bias vector\n",
    "print(W1.shape)  # (6, 100)\n",
    "\n",
    "# We need to perform the affine transformation: output = embed @ W1 + b1\n",
    "# However, we can't directly multiply embed with W1 in its current shape.\n",
    "# Current embed shape: (32, 3, 2) â†’ This represents a batch of 32 samples, each with 3 characters, each embedded into 2D.\n",
    "# So, each sample has 3 * 2 = 6 total input features. We need to flatten the last two dimensions.\n",
    "\n",
    "embed = embed.view(embed.shape[0], -1)  # Reshape to (32, 6)\n",
    "print(embed.shape)\n",
    "\n",
    "# Now we can safely perform the matrix multiplication with W1 and add the bias.\n",
    "output = embed @ W1 + b1  # Resulting shape: (32, 100)\n",
    "print(output.shape)"
   ],
   "id": "e8df23d4156cb5f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 3, 2])\n",
      "torch.Size([6, 100])\n",
      "torch.Size([228146, 6])\n",
      "torch.Size([228146, 100])\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:36:09.028348Z",
     "start_time": "2025-07-20T18:36:09.015732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Concatenating the embedding vectors along the feature dimension to get shape (32, 6)\n",
    "# For example, this manually extracts each character embedding (of dim 2) and concatenates them:\n",
    "# torch.cat([embed[:, 0, :], embed[:, 1, :], embed[:, 2, :]], dim=1)\n",
    "# print(torch.cat([embed[:, 0, :], embed[:, 1, :], embed[:, 2, :]], dim=1).shape)\n",
    "\n",
    "# This works well for a fixed block size like 3, but isn't scalable for variable-length sequences.\n",
    "# Instead, we can use torch.unbind along dim=1 to unpack the 3 blocks dynamically:\n",
    "# # This gives a tuple of tensors: (embed[:,0,:], embed[:,1,:], embed[:,2,:])\n",
    "# blocks = torch.unbind(embed, dim=1)\n",
    "# print(torch.cat(blocks, dim=1).shape)  # Concatenates to shape (32, 6)\n",
    "\n",
    "# However, thereâ€™s an even more efficient and cleaner way to achieve this using torch.view()\n",
    "# Since our original embed shape is (32, 3, 2), we can flatten the last two dims directly:\n",
    "# - This operation doesnâ€™t copy memory; it just reshapes the view of the storage.\n",
    "# - It combines the 3 characters Ã— 2-dim embeddings into a single 6-dim input per sample.\n",
    "flattened_embed = embed.view(embed.shape[0], -1)\n",
    "print(flattened_embed.shape)  # (32, 6)"
   ],
   "id": "7cdde9506b4c3636",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 6])\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Forward Pass through the MLP: Adding Non-Linearity and Output Layer ####\n",
    "\n",
    "- Suppose we have 228146 input samples (i.e., character sequences).\n",
    "- Each input has been flattened into a 6-dimensional vector after embedding.\n",
    "- The flow is as follows:\n",
    "- idx â†’ Look-up table â†’ [228146, 6] â†’ Linear Layer (W1, b1) â†’ [228146, 100] â†’ tanh â†’ [228146, 100]"
   ],
   "id": "5d216106dbd00896"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:45:22.954282Z",
     "start_time": "2025-07-20T18:45:22.807108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply first linear layer &\n",
    "# Introduce non-linearity using tanh activation\n",
    "h = torch.tanh(embed.view(embed.shape[0],6) @ W1 +b1 ) # Shape: [228146, 100]\n",
    "\n",
    "# Note on broadcasting:\n",
    "# - W1 @ input gives shape [228146, 100]\n",
    "# - b1 has shape [100]\n",
    "# - PyTorch will automatically broadcast b1 to [1, 100] â†’ [228146, 100] when adding\n",
    "# This ensures the bias is added correctly across all rows.\n",
    "\n",
    "\n",
    "# ----------Final layer of the MLP maps from hidden dimension (100) to vocab size (27)\n",
    "W2 = torch.randn(100, 27)\n",
    "b2 = torch.rand(27)\n",
    "\n",
    "# Compute logits for the next character prediction\n",
    "logits = h @ W2 + b2  # Shape: [228146, 27]\n",
    "\n",
    "# Convert logits to probabilities using softmax\n",
    "# ------------------------\n",
    "# Step 1: Exponentiate logits\n",
    "counts = logits.exp()\n",
    "\n",
    "# Step 2: Normalize across vocab dimension (dim=1) to get probability distribution\n",
    "prob = counts / counts.sum(1, keepdim=True)  # Shape: [228146, 27]\n",
    "\n",
    "# Step 2: Normalize across vocab dimension (dim=1) to get probability distribution\n",
    "prob = counts / counts.sum(1, keepdim=True)  # Shape: [228146, 27]\n",
    "# -----------------------------\n",
    "# Evaluate how well the model predicted the actual next character\n",
    "# Y contains the true next character indices (shape: [228146])\n",
    "# This line extracts the predicted probability for the correct next character\n",
    "correct_probs = prob[torch.arange(embed.shape[0]), Y]\n",
    "print(correct_probs)"
   ],
   "id": "787016a0372f0b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0307e-03, 2.7908e-05, 4.9379e-05,  ..., 4.0485e-11, 3.3935e-04,\n",
      "        1.7640e-09])\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## =========================\n",
    "## Loss Function\n",
    "## ========================="
   ],
   "id": "ecc1b38d330b15f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:45:47.572716Z",
     "start_time": "2025-07-20T18:45:47.545061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cross-entropy loss to compare predicted logits vs true next characters\n",
    "loss = -prob[torch.arange(embed.shape[0]), Y].log().mean()\n",
    "print(f\"Initial manual loss: {loss.item():.4f}\")  # This is the loss we want to minimize\n"
   ],
   "id": "9422547475e94ea1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial manual loss: 10.4039\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:47:20.236430Z",
     "start_time": "2025-07-20T18:47:20.221180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------------------\n",
    "# Let's now clean up and structure the full training flow\n",
    "# ----------------------------\n",
    "# Dataset shapes\n",
    "print(\"Dataset Shapes â€” X:\", X.shape, \"Y:\", Y.shape)\n",
    "\n",
    "# Set manual seed for reproducibility\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "# Initialize model parameters\n",
    "C = torch.rand(27, 2, generator=g)       # Embedding table: vocab_size x embedding_dim\n",
    "W1 = torch.randn((6, 100), generator=g)  # First MLP layer: input_dim x hidden_dim\n",
    "b1 = torch.randn(100, generator=g)       # Bias for layer 1\n",
    "W2 = torch.randn((100, 27), generator=g) # Output layer: hidden_dim x vocab_size\n",
    "b2 = torch.randn(27, generator=g)        # Bias for output layer\n",
    "\n",
    "# Bundle all parameters\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(\"Total Parameters:\", sum(p.nelement() for p in parameters))\n",
    "\n",
    "# ========================================\n",
    "# Efficient Loss Calculation with F.cross_entropy\n",
    "# ========================================\n",
    "\n",
    "# Manual probability calculation may overflow with large logits\n",
    "logits = torch.tensor([-5.0, 0.0, 1.0, 1000.0])\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "print(\"Naive Softmax (may overflow):\", probs)\n",
    "\n",
    "# PyTorch handles this with log-sum-exp trick:\n",
    "# subtracts max(logits) to avoid overflow\n",
    "logits_safe = logits - logits.max()\n",
    "counts = logits_safe.exp()\n",
    "probs = counts / counts.sum()\n",
    "print(\"Numerically Stable Softmax:\", probs)\n",
    "\n",
    "# Note: F.cross_entropy handles this internally for better forward & backward performance\n"
   ],
   "id": "317137c8d8f5d059",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shapes â€” X: torch.Size([228146, 3]) Y: torch.Size([228146])\n",
      "Total Parameters: 3481\n",
      "Naive Softmax (may overflow): tensor([0., 0., 0., nan])\n",
      "Numerically Stable Softmax: tensor([0., 0., 0., 1.])\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## =========================\n",
    "## Training Loop\n",
    "## ========================="
   ],
   "id": "93e4b16832265d69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:48:59.423452Z",
     "start_time": "2025-07-20T18:48:59.286529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================================\n",
    "# Training Loop\n",
    "# ========================================\n",
    "\n",
    "# Enable gradients\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "epochs = 100\n",
    "for step in range(epochs):\n",
    "\n",
    "    # Mini-batch sampling\n",
    "    ix = torch.randint(0, X.shape[0], (32,))  # Random 32 samples\n",
    "\n",
    "    # Forward pass\n",
    "    emb = C[X[ix]]                         # Shape: (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)  # Shape: (32, 100)\n",
    "    logits = h @ W2 + b2                   # Shape: (32, 27)\n",
    "    loss = F.cross_entropy(logits, Y[ix])  # Cross-entropy loss\n",
    "\n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Parameter update (SGD)\n",
    "    learning_rate = 0.1\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "# Final training loss\n",
    "print(f\"Final training loss: {loss.item():.4f}\")"
   ],
   "id": "1d5598c0a6f1587d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss: 3.1268\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ========================================\n",
    "## Final Evaluation on Full Dataset\n",
    "### ========================================"
   ],
   "id": "df0d517e241c3798"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:49:12.972158Z",
     "start_time": "2025-07-20T18:49:12.866991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "emb = C[X]                             # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "final_loss = F.cross_entropy(logits, Y)\n",
    "print(f\"Loss on full dataset: {final_loss.item():.4f}\")"
   ],
   "id": "4307dd2cb34a6f5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on full dataset: 3.1447\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  Summary of Improvements:\n",
    "- Clear headings for each section\n",
    "\n",
    "- Descriptive and concise comments explaining what each step does\n",
    "\n",
    "- Reorganized flow for better readability\n",
    "\n",
    "- Included numerical stability explanation with softmax + PyTorchâ€™s F.cross_entropy"
   ],
   "id": "8946dd48943c1a8f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
