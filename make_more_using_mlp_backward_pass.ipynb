{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Make_More V3\n",
    "#### Bulding our own loss.backward()\n"
   ],
   "id": "b9a52b674836080e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-18T12:21:33.264949Z",
     "start_time": "2025-07-18T12:21:33.249434Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "from networkx.classes import non_edges\n",
    "from sympy import hermite_prob\n",
    "from torch import nn\n",
    "# from Transformers.NanoGPT import vocab_size\n",
    "%matplotlib inline\n"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:21:34.585367Z",
     "start_time": "2025-07-18T12:21:34.570831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ],
   "id": "8a2b89a8ca48a0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:21:36.118932Z",
     "start_time": "2025-07-18T12:21:36.113826Z"
    }
   },
   "cell_type": "code",
   "source": "len(words)",
   "id": "b3d1244c552afb16",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:21:37.206254Z",
     "start_time": "2025-07-18T12:21:37.197856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ],
   "id": "fb6993bd6009f0fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:21:39.608671Z",
     "start_time": "2025-07-18T12:21:39.132155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 3\n",
    "context = [0] * block_size\n",
    "\n",
    "\n",
    "def build_datasets(words):\n",
    "    X , Y = [] , []\n",
    "\n",
    "    for w in words:\n",
    "        # print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \".\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(\"X:\" , X ,  \"Y:\" , Y)\n",
    "            # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_datasets(words[:n1])  # 80% of Xtr\n",
    "Xdev, Ydev = build_datasets(words[n1:n2]) # 10%\n",
    "Xte, Yte = build_datasets(words[n2:]) # 10%\n",
    "\n",
    "print(f'Xtr: {Xtr.shape}, Ytr: {Ytr.shape}')\n",
    "print(f'Xdev: {Xdev.shape}, Ydev: {Ydev.shape}')\n",
    "print(f'Xte: {Xte.shape}, Yte: {Yte.shape}')\n",
    "\n"
   ],
   "id": "e9260b3b47607686",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr: torch.Size([182594, 3]), Ytr: torch.Size([182594])\n",
      "Xdev: torch.Size([22846, 3]), Ydev: torch.Size([22846])\n",
      "Xte: torch.Size([22706, 3]), Yte: torch.Size([22706])\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:21:41.118017Z",
     "start_time": "2025-07-18T12:21:41.114064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function Used to comparing manual gradients with Pytorch Gradients\n",
    "def cmp(s, dt , t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ],
   "id": "99d2fe197c60b428",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:21:42.783881Z",
     "start_time": "2025-07-18T12:21:42.773145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "# -------------Embeddings --------------------\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ],
   "id": "14d868e8a0c468c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:21:45.343071Z",
     "start_time": "2025-07-18T12:21:45.336724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ],
   "id": "2a00d2eda1eca708",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:21:46.949044Z",
     "start_time": "2025-07-18T12:21:46.945701Z"
    }
   },
   "cell_type": "code",
   "source": "Xb.shape",
   "id": "d9a59467fd0d9c4e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T18:42:44.682171Z",
     "start_time": "2025-07-18T18:42:44.671275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "# ---------------Getting the Embeddings of character contex-------------\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(Xb.shape[0],-1) # concatenate the vectors\n",
    "\n",
    "# ----------------Linear Layer 1----------------------\n",
    "hprebn = embcat @ W1 +b1\n",
    "\n",
    "#---------------- Batch Norm Layer -------------------\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff*bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# -----Non-linearity----------------------\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "#---------Linear Layer 2 (27 neurons for 27 char)----------\n",
    "logits = h @ W2 + b2\n",
    "print(f'Logits:{logits.shape}')\n",
    "# ----------------cross entropy loss (same as F.cross_entropy(logits, Yb))----\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtracting logits from max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts*counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "# calculating loss from log probs\n",
    "loss = -logprobs[range(n), Yb].mean()  # this line plucks the correct next character log probs values as in Yb(index)\n",
    "print(f'Manually Calculated Loss:{loss}')\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "print(f'Pytorch calculated Loss :{loss}')"
   ],
   "id": "1e3cbfa313397566",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:torch.Size([32, 27])\n",
      "Manually Calculated Loss:3.296250343322754\n",
      "Pytorch calculated Loss :3.296250343322754\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T18:42:46.779065Z",
     "start_time": "2025-07-18T18:42:46.773923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'dlogits :{dlogits.shape}')\n",
    "print(f'H shape: {h.shape}')\n",
    "print(f'W2 shape: {W2.shape}')\n",
    "print(f'b2 shape: {b2.shape}')"
   ],
   "id": "e6ce48b24fd057ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogits :torch.Size([32, 27])\n",
      "H shape: torch.Size([32, 64])\n",
      "W2 shape: torch.Size([64, 27])\n",
      "b2 shape: torch.Size([27])\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T19:10:07.185310Z",
     "start_time": "2025-07-19T19:10:07.178934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------hprebn = embcat @ W1 +b1--------------\n",
    "print(f'hprebn :{hprebn.shape}')\n",
    "print(f'embcat: {embcat.shape}')\n",
    "print(f'W1: {W1.shape}')\n",
    "print(f'b1: {b1.shape}')\n",
    "\n",
    "dembcat =  dhprebn @ W1.T\n",
    "print(f'dembcat : {dembcat.shape}')\n",
    "dW1 = embcat.T @ dhprebn\n",
    "print(f'dW1: {dW1.shape}')\n",
    "db1 = dhprebn.sum(0)\n",
    "print(f'db1 :{db1.shape}')"
   ],
   "id": "2a159821eb230f1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn :torch.Size([32, 64])\n",
      "embcat: torch.Size([32, 30])\n",
      "W1: torch.Size([30, 64])\n",
      "b1: torch.Size([64])\n",
      "dembcat : torch.Size([32, 30])\n",
      "dW1: torch.Size([30, 64])\n",
      "db1 :torch.Size([64])\n"
     ]
    }
   ],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T18:42:48.019066Z",
     "start_time": "2025-07-18T18:42:48.013444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------ hpreact = bngain * bnraw + bnbias ------------\n",
    "print(f'hpreact :{hpreact.shape}')\n",
    "print(f'bngain :{bngain.shape}')\n",
    "print(f'bnraw : {bnraw.shape}')\n",
    "print(f'bnbias :{bnbias.shape}')"
   ],
   "id": "2b6524fe4a5f6fda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hpreact :torch.Size([32, 64])\n",
      "bngain :torch.Size([1, 64])\n",
      "bnraw : torch.Size([32, 64])\n",
      "bnbias :torch.Size([1, 64])\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T03:26:13.658305Z",
     "start_time": "2025-07-18T03:26:13.631431Z"
    }
   },
   "cell_type": "code",
   "source": "#\n",
   "id": "d7ba07a2cf7e2e69",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T18:42:49.739923Z",
     "start_time": "2025-07-18T18:42:49.731827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------backprop : logits = h @ W2 + b2 ---------------\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)"
   ],
   "id": "ec818da2b4affc32",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T18:43:11.600153Z",
     "start_time": "2025-07-18T18:43:11.591285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------bnraw = bndiff*bnvar_inv --------\n",
    "print(bnraw.shape)\n",
    "print(bndiff.shape)\n",
    "print(bnvar.shape)\n",
    "print(bnvar_inv.shape)\n",
    "# -- dbnraw/dbndiff = bnvar_inv*bnraw\n",
    "dbndiff = bnvar_inv*dbnraw\n",
    "print(dbndiff.shape)\n",
    "\n",
    "#------bnvar_inv = (bnvar**2 + 1e-5)**-0.5 --------\n",
    "dbnvar =  -0.5*(bnvar + 1e-5)**-1.5\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "fa9a790393a5e7bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([32, 64])\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T00:50:48.153024Z",
     "start_time": "2025-07-18T00:50:47.974024Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1126146e0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 60,
   "source": "plt.imshow(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]))",
   "id": "aac4e3ed9cd4835d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T19:22:40.206818Z",
     "start_time": "2025-07-19T19:22:40.179593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------- Backpropagating through the whole thing manually\n",
    "# ------- Backprop through exactly all variables\n",
    "# ------- As we defined in the forward pass one by one\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "#  probs ---> log(probs)------> logprobs ====> dprobs = 1/probs*d(output)\n",
    "dprobs = (1.0/probs)*dlogprobs # chain rule\n",
    "dcounts_sum_inv = (counts*dprobs).sum(1, keepdim=True) # summation is due to the replication of counts_sum_inv\n",
    "# ----- Since the shape of counts and counts_sum_inv are not equal there is a replication of counts_sum_inv to\n",
    "# all of columns\n",
    "# C = a*b tensors ====> dc/da = a * chainrule---------------\n",
    "# a[3x3] , b[3x1]\n",
    "# a11*b1  a12*b1 a13*b1 # here b is replicating along the all columns of a\n",
    "# a21*b2  a22*b2 a23*b2 # so for this in backward pass while calculating gradient if same node gives two outputs\n",
    "# a31*b3 a32*b3  a33*b3 # the gradients have to summ at that node\n",
    "dcounts = counts_sum_inv * dprobs # counts node is using two branches so we need to sum the gradients\n",
    "dcounts_sum = -(counts_sum**-2)*dcounts_sum_inv\n",
    "# ---- counts.sum(1, keepdim=True) here we are using sum for row summing\n",
    "#  a11 a12 a13 => b1 = a11 + a12 + a13\n",
    "#  a21 a22 a23 => b2 = a21 + a22 + a23\n",
    "#  a31 a32 a33 => b2 = a31 + a32 + a33\n",
    "dcounts += torch.ones_like(counts)*dcounts_sum\n",
    "dnorm_logits = counts*dcounts\n",
    "# ---norm_logits = logits - logit_maxes   the shape is not same (logit maxes has [32, 1] which is broadcasting 27 times\n",
    "# we have to encounter this as we do in the dcounts_sum_inv_\n",
    "# c11 c12 c13 = a11 a12 a13  - b1\n",
    "# c21 c22 c23 = a21 a22 a23  - b2 # so dlogits =  1* chain_rule ==dnorm_logits\n",
    "# c31 c32 c33 = a31 a32 a33  - b3\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes =  -(dnorm_logits).sum(1, keepdim=True)\n",
    "# -------------- logits.max(1, keepdim=True)---------\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1])*dlogit_maxes\n",
    "# --------backprop : logits = h @ W2 + b2 ---------------\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "# --------- h = torch.tanh(hpreact) --------------- dh/d(z) = 1 - (tanh(z))**2(chainrule) === z= hpreact\n",
    "dhpreact = (1- h**2)*dh\n",
    "# --------------------------hpreact = bngain * bnraw + bnbias ------------------------\n",
    "dbngain = (bnraw*dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = (bngain*dhpreact)\n",
    "dbnbias = (dhpreact).sum(0, keepdim=True)\n",
    "# -----------bnraw = bndiff*bnvar_inv -----------------\n",
    "dbndiff = bnvar_inv*dbnraw\n",
    "dbnvar_inv = (bndiff*dbnraw).sum(0, keepdim=True)\n",
    "\n",
    "#------bnvar_inv = (bnvar**2 + 1e-5)**-0.5 --------\n",
    "dbnvar =  (-0.5*(bnvar + 1e-5)**-1.5)* dbnvar_inv\n",
    "# Note: When we have a tensor.Sum() in forward pass, => turns to a replication with broadcasting in the backward pass\n",
    "# Conversely, When there is a replication due broadcast in forward pass => in backward we are going to sum the values\n",
    "\n",
    "#------------ bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) --------------------\n",
    "# example :\n",
    "# a11 a12\n",
    "# a21 a22\n",
    "# b1  b2\n",
    "# b1 = 1/(n-1)*(a11+ a21)\n",
    "# b2 = 1/(n-1)*(a12 + a22)\n",
    "dbndiff2 = 1/(n-1)*torch.ones_like(bndiff2)*dbnvar\n",
    "# ----------------- bndiff2 = bndiff**2----------------------\n",
    "dbndiff += (2*bndiff)*dbndiff2\n",
    "# ------------------- bndiff = hprebn - bnmeani --------------------\n",
    "dhprebn = dbndiff.clone() # branch 1\n",
    "dbnmeani = (-torch.ones_like(bndiff)*dbndiff).sum(0 , keepdim=True)\n",
    "# ---------------------bnmeani = 1/n*hprebn.sum(0, keepdim=True)-----------\n",
    "dhprebn += 1.0/n*(torch.ones_like(hprebn)*dbnmeani)\n",
    "\n",
    "# ------------hprebn = embcat @ W1 +b1--------------\n",
    "dembcat =  dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "# ---------embcat = emb.view(Xb.shape[0],-1) -----------------\n",
    "demb = dembcat.view(emb.shape)\n",
    "# ---------- emb = C[Xb]-----------------\n",
    "dC = torch.zeros_like(C)\n",
    "for i in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[i,j]\n",
    "        dC[ix]+= demb[i,j]\n",
    "\n",
    "\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n"
   ],
   "id": "d7bdb8e97c68bfa4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T20:32:11.179845Z",
     "start_time": "2025-07-19T20:32:11.115713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ],
   "id": "48751dc9a126cac0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.296250343322754 diff: 0.0\n"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T20:35:13.544504Z",
     "start_time": "2025-07-19T20:35:13.518454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# backward pass through F.cross_entropy\n",
    "dlogits = F.softmax(logits, dim= 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "cmp('logits', dlogits, logits) # I can have only the approximate True with diff ~ 5"
   ],
   "id": "33f5557e593adada",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 4.190951585769653e-09\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T20:41:22.243760Z",
     "start_time": "2025-07-19T20:41:22.230973Z"
    }
   },
   "cell_type": "code",
   "source": "logits.shape  , Yb.shape",
   "id": "fee56c830b619135",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32]))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T20:41:53.089763Z",
     "start_time": "2025-07-19T20:41:53.078919Z"
    }
   },
   "cell_type": "code",
   "source": "F.softmax(logits, 1)[0]",
   "id": "6a6333f4bedd25cc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0164, 0.0119, 0.0503, 0.0146, 0.0371, 0.0158, 0.0797, 0.0429, 0.0487,\n",
       "        0.0506, 0.0174, 0.0301, 0.0330, 0.0311, 0.0194, 0.0560, 0.0787, 0.0426,\n",
       "        0.0320, 0.0211, 0.0455, 0.0339, 0.0974, 0.0078, 0.0220, 0.0288, 0.0351],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T20:42:40.605243Z",
     "start_time": "2025-07-19T20:42:40.596634Z"
    }
   },
   "cell_type": "code",
   "source": "dlogits[0]*n",
   "id": "4eaf1ff54cfe096e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0164,  0.0119,  0.0503,  0.0146,  0.0371, -0.9842,  0.0797,  0.0429,\n",
       "         0.0487,  0.0506,  0.0174,  0.0301,  0.0330,  0.0311,  0.0194,  0.0560,\n",
       "         0.0787,  0.0426,  0.0320,  0.0211,  0.0455,  0.0339,  0.0974,  0.0078,\n",
       "         0.0220,  0.0288,  0.0351], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T20:44:21.071556Z",
     "start_time": "2025-07-19T20:44:21.059959Z"
    }
   },
   "cell_type": "code",
   "source": "dlogits[0].sum()",
   "id": "aacfa1f8af618295",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.3283e-09, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T20:45:32.728561Z",
     "start_time": "2025-07-19T20:45:32.538007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ],
   "id": "5fed70890b6ade7b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10b154910>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:06:30.016138Z",
     "start_time": "2025-07-19T22:06:29.982758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ],
   "id": "fc383b873a8ba09d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:11:04.298433Z",
     "start_time": "2025-07-19T22:11:04.280981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "dhprebn = bngain*bnvar_inv/n*(n*dhpreact -dhpreact.sum(0)- n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "cmp('hprebn', dhprebn, hprebn)"
   ],
   "id": "946afe2c316ad0f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:35:17.118300Z",
     "start_time": "2025-07-19T22:33:20.083763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Finally lets put all the code together\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "\n",
    "# --------------Backward Pass-------------------\n",
    "with torch.no_grad():\n",
    "    # Let's start the optimization\n",
    "    for i in range(max_steps):\n",
    "        # minibatch construct\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "        # forward pass\n",
    "        emb = C[Xb] # embed the characters into vectors\n",
    "        embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "        # Linear layer\n",
    "        hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "        # BatchNorm layer\n",
    "        # -------------------------------------------------------------\n",
    "        bnmean = hprebn.mean(0, keepdim=True)\n",
    "        bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "        bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "        bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "        hpreact = bngain * bnraw + bnbias\n",
    "        # -------------------------------------------------------------\n",
    "        # Non-linearity\n",
    "        h = torch.tanh(hpreact) # hidden layer\n",
    "        logits = h @ W2 + b2 # output layer\n",
    "        loss = F.cross_entropy(logits, Yb) # loss function\n",
    "        # backward pass\n",
    "        for p in parameters:\n",
    "          p.grad = None\n",
    "        #loss.backward() # use this for correctness\n",
    "        #------------------\n",
    "        # manual backprop! as we derive from above\n",
    "        # -----------------(Backward Pass of Cross Entropy)-----------------\n",
    "        dlogits = F.softmax(logits, 1)\n",
    "        dlogits[range(n), Yb] -= 1\n",
    "        dlogits /= n\n",
    "        # 2nd layer backpropagation\n",
    "        dh = dlogits @ W2.T\n",
    "        dW2 = h.T @ dlogits\n",
    "        db2 = dlogits.sum(0)\n",
    "       # tanh layer\n",
    "        dhpreact = (1.0 - h**2) * dh\n",
    "        # batchnorm backprop as we derive from equation\n",
    "        dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "        dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "        dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "        # 1st layer\n",
    "        dembcat = dhprebn @ W1.T\n",
    "        dW1 = embcat.T @ dhprebn\n",
    "        db1 = dhprebn.sum(0)\n",
    "        # embedding\n",
    "        demb = dembcat.view(emb.shape)\n",
    "        dC = torch.zeros_like(C)\n",
    "        for k in range(Xb.shape[0]):\n",
    "          for j in range(Xb.shape[1]):\n",
    "            ix = Xb[k,j]\n",
    "            dC[ix] += demb[k,j]\n",
    "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "        #---------End Of Backward Prop--------------------------------------------------------------------------------\n",
    "\n",
    "        # update\n",
    "        lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "        for p, grad in zip(parameters, grads):\n",
    "          #p.data += -lr * p.grad #using PyTorch grad from .backward()\n",
    "          p.data += -lr * grad # new way of manual backward prop\n",
    "        # track stats\n",
    "        if i % 10000 == 0: # print every once in a while\n",
    "          print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "        lossi.append(loss.log10().item())\n",
    "        # if i >= 100:\n",
    "        #     break"
   ],
   "id": "42f194f4bb996509",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.6642\n",
      "  10000/ 200000: 2.0908\n",
      "  20000/ 200000: 1.9414\n",
      "  30000/ 200000: 2.4209\n",
      "  40000/ 200000: 2.0367\n",
      "  50000/ 200000: 2.0508\n",
      "  60000/ 200000: 1.7356\n",
      "  70000/ 200000: 2.1609\n",
      "  80000/ 200000: 2.3147\n",
      "  90000/ 200000: 1.9014\n",
      " 100000/ 200000: 1.9420\n",
      " 110000/ 200000: 1.7423\n",
      " 120000/ 200000: 2.5159\n",
      " 130000/ 200000: 2.4714\n",
      " 140000/ 200000: 2.3367\n",
      " 150000/ 200000: 2.3549\n",
      " 160000/ 200000: 2.1146\n",
      " 170000/ 200000: 2.2802\n",
      " 180000/ 200000: 1.7952\n",
      " 190000/ 200000: 2.2002\n"
     ]
    }
   ],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:32:16.154670Z",
     "start_time": "2025-07-19T22:32:15.958504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ],
   "id": "1187c3d4fc7f7ae5",
   "outputs": [],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:32:04.119343Z",
     "start_time": "2025-07-19T22:32:04.116892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# useful for checking your gradients\n",
    "# for p,g in zip(parameters, grads):\n",
    "#   cmp(str(tuple(p.shape)), g, p)"
   ],
   "id": "3624f01825590651",
   "outputs": [],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:32:31.158703Z",
     "start_time": "2025-07-19T22:32:30.942131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ],
   "id": "3efac3a294217165",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.6772406101226807\n",
      "val 2.6815273761749268\n"
     ]
    }
   ],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:32:45.025318Z",
     "start_time": "2025-07-19T22:32:44.996934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # ------------\n",
    "      # forward pass:\n",
    "      # Embedding\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # ------------\n",
    "      # Sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ],
   "id": "ddb4ef25f8888edb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmahxqtellqvufi.\n",
      "mrixreaty.\n",
      "hlcadsn.\n",
      "jarhnteflpliart.\n",
      "kaqei.\n",
      "jerania.\n",
      "coriiv.\n",
      "kane.\n",
      "gph.\n",
      "bmaioi.\n",
      "dqsnnn.\n",
      "sroivwa.\n",
      "jadiq.\n",
      "warelogiearyxi.\n",
      "fkcenpdvsab.\n",
      "ed.\n",
      "ecoia.\n",
      "gtlef.\n",
      "yar.\n",
      "aulaag.\n"
     ]
    }
   ],
   "execution_count": 150
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
